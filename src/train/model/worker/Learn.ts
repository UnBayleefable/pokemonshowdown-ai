import * as tf from "@tensorflow/tfjs";
import {LearnConfig} from "../../../config/types";
import {intToChoice} from "../../../psbot/handlers/battle/agent";
import {Metrics} from "./Metrics";
import {BatchedExample} from "./dataset";

/**
 * Encapsulates the learning step of training, where the model is updated based
 * on experience generated by rollout games.
 */
export class Learn {
    /** Metrics logger. */
    private readonly metrics = Metrics.get(`${this.name}/learn`);
    /** Used for calculating gradients. */
    private readonly optimizer = tf.train.sgd(this.config.learningRate);
    /** Collection of trainable variables in the model. */
    private readonly variables = this.model.trainableWeights.map(
        w => w.read() as tf.Variable,
    );
    /** Used for logging inputs during loss calcs. */
    private readonly denseLayers: readonly tf.layers.Layer[] =
        this.model.layers.filter(l => l.getClassName() === "Dense");

    /**
     * Creates a Learn object.
     *
     * @param name Name of the training run for logging.
     * @param model Model to train.
     * @param iterator Iterator to pull from to obtain batched experiences for
     * learning.
     * @param config Learning config.
     */
    public constructor(
        public readonly name: string,
        private readonly model: tf.LayersModel,
        private readonly iterator: AsyncIterator<BatchedExample>,
        private readonly config: LearnConfig,
    ) {
        // Log initial weights.
        for (const weights of this.variables) {
            this.metrics?.histogram(`${weights.name}/weights`, weights, 0);
        }
    }

    /**
     * Performs the configured amount of batch update steps on the model,
     * completing one learning episode.
     *
     * @param step Episode step number for logging.
     * @param callback Called for each batch.
     * @returns The average loss of each batch update.
     */
    public async episode(
        step: number,
        callback?: (step: number, loss: number) => void,
    ): Promise<number> {
        let avgLoss = tf.scalar(0, "float32");
        const totalGrads: tf.NamedTensorMap = {};
        const avgInputs: tf.NamedTensorMap = {};

        for (const layer of this.denseLayers) {
            // Note: Call hook is wrapped in tf.tidy() so tf.keep() is used.
            layer.setCallHook(function logInputs(inputs) {
                if (!Array.isArray(inputs)) {
                    inputs = [inputs];
                }
                for (let i = 0; i < inputs.length; ++i) {
                    // Average along all axes except last one in order to
                    // account for vectorized inputs.
                    const input = inputs[i]
                        .mean(inputs[i].shape.map((_, j) => j).slice(0, -1))
                        .flatten();
                    const name =
                        inputs.length > 1 ? `${layer.name}/${i}` : layer.name;
                    if (Object.prototype.hasOwnProperty.call(avgInputs, name)) {
                        const oldInput = avgInputs[name];
                        avgInputs[name] = tf.keep(tf.add(oldInput, input));
                        tf.dispose(oldInput);
                    } else {
                        avgInputs[name] = tf.keep(input);
                    }
                }
            });
        }

        for (let i = 0; i < this.config.updates; ++i) {
            const {batchLoss, batchGrads} = await this.update();
            callback?.(i + 1, (await batchLoss.data<"float32">())[0]);

            const oldAvgLoss = avgLoss;
            avgLoss = tf.add(oldAvgLoss, batchLoss);
            tf.dispose([oldAvgLoss, batchLoss]);

            for (const name in batchGrads) {
                if (!Object.prototype.hasOwnProperty.call(batchGrads, name)) {
                    continue;
                }
                if (Object.prototype.hasOwnProperty.call(totalGrads, name)) {
                    const oldGrads = totalGrads[name];
                    totalGrads[name] = tf.add(oldGrads, batchGrads[name]);
                    tf.dispose([oldGrads, batchGrads[name]]);
                } else {
                    totalGrads[name] = batchGrads[name];
                }
            }
        }
        const oldAvgLoss = avgLoss;
        avgLoss = tf.div(oldAvgLoss, this.config.updates);
        tf.dispose(oldAvgLoss);

        const avgLossData = await avgLoss.data<"float32">();
        this.metrics?.scalar("loss", avgLoss, step);
        tf.dispose(avgLoss);

        for (const name in totalGrads) {
            if (!Object.prototype.hasOwnProperty.call(totalGrads, name)) {
                continue;
            }
            this.metrics?.histogram(`${name}/grads`, totalGrads[name], step);
            tf.dispose(totalGrads[name]);
        }

        for (const name in avgInputs) {
            if (!Object.prototype.hasOwnProperty.call(avgInputs, name)) {
                continue;
            }
            const oldInput = avgInputs[name];
            avgInputs[name] = tf.div(oldInput, this.config.updates);
            this.metrics?.histogram(`${name}/input`, avgInputs[name], step);
            tf.dispose([oldInput, avgInputs[name]]);
        }

        for (const weights of this.variables) {
            this.metrics?.histogram(`${weights.name}/weights`, weights, step);
        }

        for (const layer of this.model.layers) {
            layer.clearCallHook();
        }

        return avgLossData[0];
    }

    /** Performs one batch update step, returning the loss. */
    private async update(): Promise<{
        batchLoss: tf.Scalar;
        batchGrads: tf.NamedTensorMap;
    }> {
        const result = await this.iterator.next();
        if (result.done) {
            throw new Error("No more data in dataset");
        }
        const batch = result.value;

        // Dataset batching process turns arrays into objs so need to undo this.
        const batchState: tf.Tensor[] = [];
        for (const key of Object.keys(batch.state)) {
            const index = Number(key);
            batchState[index] = batch.state[index];
        }

        // Compute batch gradients manually to be able to do logging in-between.
        const {value: batchLoss, grads: batchGrads} =
            this.optimizer.computeGradients(
                () => this.loss(batchState, batch.action, batch.returns),
                this.variables,
            );
        tf.dispose(batch);
        this.optimizer.applyGradients(batchGrads);

        return {batchLoss, batchGrads};
    }

    private loss(
        state: tf.Tensor[],
        action: tf.Tensor,
        returns: tf.Tensor,
    ): tf.Scalar {
        const {model} = this;
        return tf.tidy("loss", function lossImpl() {
            // Isolate the Q-value of the action that was taken.
            const output = model.predictOnBatch(state) as tf.Tensor;
            const mask = tf.oneHot(action, intToChoice.length);
            const q = tf.sum(tf.mul(output, mask), -1);

            // Compute the loss based on the discount reward actually obtained
            // from that action.
            return tf.losses.meanSquaredError(returns, q);
        });
    }

    /** Cleans up dangling variables. */
    public cleanup(): void {
        this.optimizer.dispose();
        Metrics.flush();
    }
}
